model_name: mistralai/Mistral-7B-Instruct-v0.2
dataset_path: training_pipeline/data/aether_train.jsonl
output_dir: training_pipeline/artifacts/lora-aether
max_seq_length: 1024
learning_rate: 0.0002
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
lora:
  r: 16
  alpha: 32
  dropout: 0.05
